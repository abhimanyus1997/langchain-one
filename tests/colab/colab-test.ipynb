{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain[llm] is already installed.\n",
      "gpt4all is already installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain[llm]\n",
    "%pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH: ../models/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# Define the URL, directory, and filename for your model\n",
    "MODEL_URL = \"https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/blob/main/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_FILENAME = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_PATH = None  # Define MODEL_PATH with a default value\n",
    "\n",
    "# Check if the code is running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set MODEL_PATH based on the environment\n",
    "if IN_COLAB:\n",
    "    # Code is running in Google Colab\n",
    "    MODEL_PATH = Path(MODEL_DIR) / MODEL_FILENAME\n",
    "else:\n",
    "    # Determine the current operating system\n",
    "    current_os = platform.system()\n",
    "    if current_os == \"Windows\":\n",
    "        # Convert the path to a string with forward slashes and add '..' before MODEL_DIR\n",
    "        MODEL_PATH = (Path(\"..\") / MODEL_DIR / MODEL_FILENAME).as_posix()\n",
    "    else:\n",
    "        # Convert the path to a string with forward slashes and add '..' before MODEL_DIR\n",
    "        MODEL_PATH = (Path(\"..\") / MODEL_DIR / MODEL_FILENAME).as_posix()\n",
    "\n",
    "# Check if MODEL_PATH is defined\n",
    "if MODEL_PATH is not None:\n",
    "    print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "    # You can use MODEL_PATH to access the path to your model file.\n",
    "else:\n",
    "    print(\"MODEL_PATH is not defined. Please check your environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Model\n",
      "Model file already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Heading: Download Model\n",
    "print(\"Download Model\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the model file if it doesn't exist locally.\n",
    "    \"\"\"\n",
    "    # Check if the model file already exists\n",
    "    if MODEL_PATH and not os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "            print(f\"Downloading model from {MODEL_URL}...\")\n",
    "            response = requests.get(MODEL_URL, stream=True)  # Use streaming to download in chunks\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "\n",
    "            # Create a progress bar using tqdm\n",
    "            with open(MODEL_PATH, 'wb') as file, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "                    pbar.update(len(data))\n",
    "\n",
    "            print(f\"Downloaded model to {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Model file already exists. Skipping download.\")\n",
    "\n",
    "# Call the download_model function to download the model if necessary\n",
    "download_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../models/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\projects\\langchain-one\\tests\\test.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m llm \u001b[39m=\u001b[39m GPT4All(model\u001b[39m=\u001b[39;49mMODEL_PATH, callbacks\u001b[39m=\u001b[39;49mcallbacks, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[0;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\langchain\\llms\\gpt4all.py:142\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    139\u001b[0m model_path, delimiter, model_name \u001b[39m=\u001b[39m full_path\u001b[39m.\u001b[39mrpartition(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    140\u001b[0m model_path \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delimiter\n\u001b[1;32m--> 142\u001b[0m values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m GPT4AllModel(\n\u001b[0;32m    143\u001b[0m     model_name,\n\u001b[0;32m    144\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path \u001b[39mor\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    145\u001b[0m     model_type\u001b[39m=\u001b[39;49mvalues[\u001b[39m\"\u001b[39;49m\u001b[39mbackend\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    146\u001b[0m     allow_download\u001b[39m=\u001b[39;49mvalues[\u001b[39m\"\u001b[39;49m\u001b[39mallow_download\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    147\u001b[0m     device\u001b[39m=\u001b[39;49mvalues[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m values[\u001b[39m\"\u001b[39m\u001b[39mn_threads\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     \u001b[39m# set n_threads\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mset_thread_count(values[\u001b[39m\"\u001b[39m\u001b[39mn_threads\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\gpt4all\\gpt4all.py:98\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     97\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minit_gpu(model_path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m], device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     99\u001b[0m \u001b[39m# Set n_threads\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m n_threads \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\gpt4all\\pyllmodel.py:265\u001b[0m, in \u001b[0;36mLLModel.load_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m llmodel\u001b[39m.\u001b[39mllmodel_model_create(model_path_enc)\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     llmodel\u001b[39m.\u001b[39;49mllmodel_loadModel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, model_path_enc)\n\u001b[0;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to instantiate model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=str(MODEL_PATH), callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Boil water in a pot.\n",
      "2. Add loose leaf tea (e.g. green tea, jasmine tea) into the pot.\n",
      "3. Let it steep for 5-10 minutes.\n",
      "4. Strain the tea into another pot.\n",
      "5. Add milk and sweetener (e.g. sugar, honey) to taste.\n",
      "6. Heat the mixture on low heat until it's warm.\n",
      "7. Pour the tea into a cup and enjoy!"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\"How to make milk tea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'text'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\projects\\langchain-one\\tests\\test.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mAsk me anything:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m llm_chain(\u001b[39mstr\u001b[39m(\u001b[39minput\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m answer \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "input = input(\"Ask me anything:\")\n",
    "output = llm_chain(str(input))\n",
    "answer = output['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: what are major machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "print(f\"You asked: {output['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
