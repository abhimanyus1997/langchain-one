{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain[llm] is already installed.\n",
      "gpt4all is already installed.\n"
     ]
    }
   ],
   "source": [
    "# Check if langchain[llm] is installed\n",
    "try:\n",
    "    import langchain\n",
    "    print(\"langchain[llm] is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"langchain[llm] is not installed. Installing...\")\n",
    "    %pip install langchain[llm]\n",
    "\n",
    "# Check if gpt4all is installed\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "    print(\"gpt4all is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"gpt4all is not installed. Installing...\")\n",
    "    %pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_FILENAME = \"mistral-7b-openorca.Q4_0.gguf\"\n",
    "MODEL_PATH = None  # Define MODEL_PATH with a default value\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Code is running in Google Colab\n",
    "    MODEL_PATH = Path(MODEL_DIR) / MODEL_FILENAME\n",
    "else:\n",
    "    # Determine the current operating system\n",
    "    current_os = platform.system()\n",
    "    if current_os == \"Windows\":\n",
    "        # Convert the path to a string with forward slashes and add '..' before MODEL_DIR\n",
    "        MODEL_PATH = (Path(\"..\") / MODEL_DIR / MODEL_FILENAME).as_posix()\n",
    "    else:\n",
    "        # Convert the path to a string with forward slashes and add '..' before MODEL_DIR\n",
    "        MODEL_PATH = (Path(\"..\") / MODEL_DIR / MODEL_FILENAME).as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Model\n",
      "Model file already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Heading: Download Model\n",
    "print(\"Download Model\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the model file if it doesn't exist locally.\n",
    "    \"\"\"\n",
    "    # Check if the model file already exists\n",
    "    if MODEL_PATH and not os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "            print(f\"Downloading model from {MODEL_URL}...\")\n",
    "            response = requests.get(MODEL_URL, stream=True)  # Use streaming to download in chunks\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "\n",
    "            # Create a progress bar using tqdm\n",
    "            with open(MODEL_PATH, 'wb') as file, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "                    pbar.update(len(data))\n",
    "\n",
    "            print(f\"Downloaded model to {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Model file already exists. Skipping download.\")\n",
    "\n",
    "# Call the download_model function to download the model if necessary\n",
    "download_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../models/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=MODEL_PATH, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Boil water in a pot.\n",
      "2. Add loose leaf tea (e.g. green tea, jasmine tea) into the pot.\n",
      "3. Let it steep for 5-10 minutes.\n",
      "4. Strain the tea into another pot.\n",
      "5. Add milk and sweetener (e.g. sugar, honey) to taste.\n",
      "6. Heat the mixture on low heat until it's warm.\n",
      "7. Pour the tea into a cup and enjoy!"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\"How to make milk tea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'text'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Support Vector Machines (SVM)\n",
      "2. Artificial Neural Networks (ANN)\n",
      "3. Decision Trees\n",
      "4. Random Forests\n",
      "5. Gradient Boosting Machines (GBM)\n",
      "6. K-Nearest Neighbors (KNN)\n",
      "7. Principal Component Analysis (PCA)\n",
      "8. Clustering Algorithms\n",
      "9. Deep Learning\n",
      "10. Reinforcement Learning\n",
      "11. Transfer Learning\n",
      "12. Online Learning\n",
      "13. Semi-Supervised Learning\n",
      "14. Unsupervised Learning\n",
      "15. Reinforcement Learning\n",
      "16. Distributed Machine Learning\n",
      "17. Big Data Machine Learning\n",
      "18. Explainable Artificial Intelligence (AI)\n",
      "19. Explainable Deep Learning (DL)\n",
      "20. Explainable Natural Language Processing (NLP)\n",
      "21. Explainable Computer Vision (CV)\n",
      "22. Explainable Robotics\n",
      "23. Explainable IoT\n",
      "24. Explainable AI for Healthcare\n",
      "25. Explainable AI for Finance\n",
      "26. Explainable AI for Supply Chain\n",
      "27. Explainable AI for Marketing\n",
      "28. Explainable AI for Education\n",
      "29. Explainable AI for Government\n",
      "30. Explainable AI for Media\n",
      "31. Explainable AI for\n",
      "\n",
      "1. Support Vector Machines (SVM)\n",
      "2. Artificial Neural Networks (ANN)\n",
      "3. Decision Trees\n",
      "4. Random Forests\n",
      "5. Gradient Boosting Machines (GBM)\n",
      "6. K-Nearest Neighbors (KNN)\n",
      "7. Principal Component Analysis (PCA)\n",
      "8. Clustering Algorithms\n",
      "9. Deep Learning\n",
      "10. Reinforcement Learning\n",
      "11. Transfer Learning\n",
      "12. Online Learning\n",
      "13. Semi-Supervised Learning\n",
      "14. Unsupervised Learning\n",
      "15. Reinforcement Learning\n",
      "16. Distributed Machine Learning\n",
      "17. Big Data Machine Learning\n",
      "18. Explainable Artificial Intelligence (AI)\n",
      "19. Explainable Deep Learning (DL)\n",
      "20. Explainable Natural Language Processing (NLP)\n",
      "21. Explainable Computer Vision (CV)\n",
      "22. Explainable Robotics\n",
      "23. Explainable IoT\n",
      "24. Explainable AI for Healthcare\n",
      "25. Explainable AI for Finance\n",
      "26. Explainable AI for Supply Chain\n",
      "27. Explainable AI for Marketing\n",
      "28. Explainable AI for Education\n",
      "29. Explainable AI for Government\n",
      "30. Explainable AI for Media\n",
      "31. Explainable AI for\n"
     ]
    }
   ],
   "source": [
    "input = input(\"Ask me anything:\")\n",
    "output = llm_chain(str(input))\n",
    "answer = output['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: what are major machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "print(f\"You asked: {output['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
