{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain[llm] is already installed.\n",
      "gpt4all is already installed.\n"
     ]
    }
   ],
   "source": [
    "# Check if langchain[llm] is installed\n",
    "try:\n",
    "    import langchain\n",
    "    print(\"langchain[llm] is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"langchain[llm] is not installed. Installing...\")\n",
    "    %pip install langchain[llm]\n",
    "\n",
    "# Check if gpt4all is installed\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "    print(\"gpt4all is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"gpt4all is not installed. Installing...\")\n",
    "    %pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/blob/main/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_FILENAME = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_PATH = Path(\"..\") / MODEL_DIR / MODEL_FILENAME\n",
    "\n",
    "# Determine the current operating system\n",
    "current_os = platform.system()\n",
    "if current_os == \"Windows\":\n",
    "    # Convert to a string with backslashes for Windows\n",
    "    model_path_str = MODEL_PATH.as_posix()\n",
    "else:\n",
    "    # Convert to a string with forward slashes for other platforms (e.g., Ubuntu)\n",
    "    model_path_str = MODEL_PATH.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Model\n",
      "Model file already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Heading: Download Model\n",
    "print(\"Download Model\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the model file if it doesn't exist locally.\n",
    "    \"\"\"\n",
    "    # Check if the model file already exists\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "            print(f\"Downloading model from {MODEL_URL}...\")\n",
    "            response = requests.get(MODEL_URL, stream=True)  # Use streaming to download in chunks\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "\n",
    "            # Create a progress bar using tqdm\n",
    "            with open(MODEL_PATH, 'wb') as file, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "                    pbar.update(len(data))\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            if not os.path.exists(MODEL_DIR):\n",
    "                os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "            print(f\"Downloaded model to {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Model file already exists. Skipping download.\")\n",
    "\n",
    "# Call the download_model function to download the model if necessary\n",
    "download_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../models/ggml-model-gpt4all-falcon-q4_0.bin')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ../models/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=model_path_str, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Boil water in a pot.\n",
      "2. Add loose leaf tea (e.g. green tea, jasmine tea) into the pot.\n",
      "3. Let it steep for 5-10 minutes.\n",
      "4. Strain the tea into another pot.\n",
      "5. Add milk and sweetener (e.g. sugar, honey) to taste.\n",
      "6. Heat the mixture on low heat until it's warm.\n",
      "7. Pour the tea into a cup and enjoy!"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\"How to make milk tea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'text'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Check for redundant features\n",
      "2. Check for irrelevant features\n",
      "3. Check for overlapping features\n",
      "4. Check for redundant values\n",
      "5. Check for irrelevant values\n",
      "6. Check for overlapping values\n",
      "7. Check for redundant values\n",
      "8. Check for irrelevant values\n",
      "9. Check for overlapping values\n",
      "10. Check for redundant values\n",
      "11. Check for irrelevant values\n",
      "12. Check for overlapping values\n",
      "13. Check for redundant values\n",
      "14. Check for irrelevant values\n",
      "15. Check for overlapping values\n",
      "16. Check for redundant values\n",
      "17. Check for irrelevant values\n",
      "18. Check for overlapping values\n",
      "19. Check for redundant values\n",
      "20. Check for irrelevant values\n",
      "21. Check for overlapping values\n",
      "22. Check for redundant values\n",
      "23. Check for irrelevant values\n",
      "24. Check for overlapping values\n",
      "25. Check for redundant values\n",
      "26. Check for irrelevant values\n",
      "27. Check for overlapping values\n",
      "28. Check for redundant values\n",
      "29. Check for irrelevant values\n",
      "30. Check for overlapping values\n",
      "31. Check for redundant values\n",
      "32. Check for irrelevant values\n",
      "33. Check for overlapping values\n",
      "34. Check for redundant values\n",
      "35. Check for irrelevant values\n",
      "36. Check for overlapping values\n",
      "37.\n",
      "\n",
      "1. Check for redundant features\n",
      "2. Check for irrelevant features\n",
      "3. Check for overlapping features\n",
      "4. Check for redundant values\n",
      "5. Check for irrelevant values\n",
      "6. Check for overlapping values\n",
      "7. Check for redundant values\n",
      "8. Check for irrelevant values\n",
      "9. Check for overlapping values\n",
      "10. Check for redundant values\n",
      "11. Check for irrelevant values\n",
      "12. Check for overlapping values\n",
      "13. Check for redundant values\n",
      "14. Check for irrelevant values\n",
      "15. Check for overlapping values\n",
      "16. Check for redundant values\n",
      "17. Check for irrelevant values\n",
      "18. Check for overlapping values\n",
      "19. Check for redundant values\n",
      "20. Check for irrelevant values\n",
      "21. Check for overlapping values\n",
      "22. Check for redundant values\n",
      "23. Check for irrelevant values\n",
      "24. Check for overlapping values\n",
      "25. Check for redundant values\n",
      "26. Check for irrelevant values\n",
      "27. Check for overlapping values\n",
      "28. Check for redundant values\n",
      "29. Check for irrelevant values\n",
      "30. Check for overlapping values\n",
      "31. Check for redundant values\n",
      "32. Check for irrelevant values\n",
      "33. Check for overlapping values\n",
      "34. Check for redundant values\n",
      "35. Check for irrelevant values\n",
      "36. Check for overlapping values\n",
      "37.\n"
     ]
    }
   ],
   "source": [
    "input = input(\"Ask me anything:\")\n",
    "output = llm_chain(str(input))\n",
    "answer = output['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: How to reduce no of features in a dataset?\n"
     ]
    }
   ],
   "source": [
    "print(f\"You asked: {output['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
