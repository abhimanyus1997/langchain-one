{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain[llm] is already installed.\n",
      "gpt4all is already installed.\n"
     ]
    }
   ],
   "source": [
    "# Check if langchain[llm] is installed\n",
    "try:\n",
    "    import langchain\n",
    "    print(\"langchain[llm] is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"langchain[llm] is not installed. Installing...\")\n",
    "    %pip install langchain[llm]\n",
    "\n",
    "# Check if gpt4all is installed\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "    print(\"gpt4all is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"gpt4all is not installed. Installing...\")\n",
    "    %pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/blob/main/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_FILENAME = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Code is running in Google Colab\n",
    "    MODEL_PATH = Path(MODEL_DIR) / MODEL_FILENAME\n",
    "else:\n",
    "    # Determine the current operating system\n",
    "    current_os = platform.system()\n",
    "\n",
    "if current_os == \"Windows\":\n",
    "    # Convert to a string with backslashes for Windows and add '..' before MODEL_DIR\n",
    "    model_path_str = \"..\" + str(Path(MODEL_DIR) / MODEL_FILENAME).as_posix()\n",
    "else:\n",
    "    # Convert to a string with forward slashes for other platforms (e.g., Ubuntu) and add '..' before MODEL_DIR\n",
    "    model_path_str = \"..\" + str(Path(MODEL_DIR) / MODEL_FILENAME).as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/ggml-model-gpt4all-falcon-q4_0.bin'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Model\n",
      "Model file already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Heading: Download Model\n",
    "print(\"Download Model\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the model file if it doesn't exist locally.\n",
    "    \"\"\"\n",
    "    # Check if the model file already exists\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "            print(f\"Downloading model from {MODEL_URL}...\")\n",
    "            response = requests.get(MODEL_URL, stream=True)  # Use streaming to download in chunks\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "\n",
    "            # Create a progress bar using tqdm\n",
    "            with open(MODEL_PATH, 'wb') as file, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "                    pbar.update(len(data))\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            if not os.path.exists(MODEL_DIR):\n",
    "                os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "            print(f\"Downloaded model to {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Model file already exists. Skipping download.\")\n",
    "\n",
    "# Call the download_model function to download the model if necessary\n",
    "download_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4All\n__root__\n  Invalid model directory: models/ (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32me:\\projects\\langchain-one\\tests\\test.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m callbacks \u001b[39m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projects/langchain-one/tests/test.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m llm \u001b[39m=\u001b[39m GPT4All(model\u001b[39m=\u001b[39;49mmodel_path_str, callbacks\u001b[39m=\u001b[39;49mcallbacks, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32me:\\projects\\langchain-one\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for GPT4All\n__root__\n  Invalid model directory: models/ (type=value_error)"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=model_path_str, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Boil water in a pot.\n",
      "2. Add loose leaf tea (e.g. green tea, jasmine tea) into the pot.\n",
      "3. Let it steep for 5-10 minutes.\n",
      "4. Strain the tea into another pot.\n",
      "5. Add milk and sweetener (e.g. sugar, honey) to taste.\n",
      "6. Heat the mixture on low heat until it's warm.\n",
      "7. Pour the tea into a cup and enjoy!"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\"How to make milk tea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'text'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Check for redundant features\n",
      "2. Check for irrelevant features\n",
      "3. Check for overlapping features\n",
      "4. Check for redundant values\n",
      "5. Check for irrelevant values\n",
      "6. Check for overlapping values\n",
      "7. Check for redundant values\n",
      "8. Check for irrelevant values\n",
      "9. Check for overlapping values\n",
      "10. Check for redundant values\n",
      "11. Check for irrelevant values\n",
      "12. Check for overlapping values\n",
      "13. Check for redundant values\n",
      "14. Check for irrelevant values\n",
      "15. Check for overlapping values\n",
      "16. Check for redundant values\n",
      "17. Check for irrelevant values\n",
      "18. Check for overlapping values\n",
      "19. Check for redundant values\n",
      "20. Check for irrelevant values\n",
      "21. Check for overlapping values\n",
      "22. Check for redundant values\n",
      "23. Check for irrelevant values\n",
      "24. Check for overlapping values\n",
      "25. Check for redundant values\n",
      "26. Check for irrelevant values\n",
      "27. Check for overlapping values\n",
      "28. Check for redundant values\n",
      "29. Check for irrelevant values\n",
      "30. Check for overlapping values\n",
      "31. Check for redundant values\n",
      "32. Check for irrelevant values\n",
      "33. Check for overlapping values\n",
      "34. Check for redundant values\n",
      "35. Check for irrelevant values\n",
      "36. Check for overlapping values\n",
      "37.\n",
      "\n",
      "1. Check for redundant features\n",
      "2. Check for irrelevant features\n",
      "3. Check for overlapping features\n",
      "4. Check for redundant values\n",
      "5. Check for irrelevant values\n",
      "6. Check for overlapping values\n",
      "7. Check for redundant values\n",
      "8. Check for irrelevant values\n",
      "9. Check for overlapping values\n",
      "10. Check for redundant values\n",
      "11. Check for irrelevant values\n",
      "12. Check for overlapping values\n",
      "13. Check for redundant values\n",
      "14. Check for irrelevant values\n",
      "15. Check for overlapping values\n",
      "16. Check for redundant values\n",
      "17. Check for irrelevant values\n",
      "18. Check for overlapping values\n",
      "19. Check for redundant values\n",
      "20. Check for irrelevant values\n",
      "21. Check for overlapping values\n",
      "22. Check for redundant values\n",
      "23. Check for irrelevant values\n",
      "24. Check for overlapping values\n",
      "25. Check for redundant values\n",
      "26. Check for irrelevant values\n",
      "27. Check for overlapping values\n",
      "28. Check for redundant values\n",
      "29. Check for irrelevant values\n",
      "30. Check for overlapping values\n",
      "31. Check for redundant values\n",
      "32. Check for irrelevant values\n",
      "33. Check for overlapping values\n",
      "34. Check for redundant values\n",
      "35. Check for irrelevant values\n",
      "36. Check for overlapping values\n",
      "37.\n"
     ]
    }
   ],
   "source": [
    "input = input(\"Ask me anything:\")\n",
    "output = llm_chain(str(input))\n",
    "answer = output['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: How to reduce no of features in a dataset?\n"
     ]
    }
   ],
   "source": [
    "print(f\"You asked: {output['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
