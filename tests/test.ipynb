{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain[llm]",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/blob/main/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_DIR = \"models\"\n",
    "MODEL_FILENAME = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "MODEL_PATH = os.path.join(\"..\",MODEL_DIR, MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Model\n",
      "Model file already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Heading: Download Model\n",
    "print(\"Download Model\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the model file if it doesn't exist locally.\n",
    "    \"\"\"\n",
    "    # Check if the model file already exists\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "            print(f\"Downloading model from {MODEL_URL}...\")\n",
    "            response = requests.get(MODEL_URL, stream=True)  # Use streaming to download in chunks\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            block_size = 1024  # 1 KB\n",
    "\n",
    "            # Create a progress bar using tqdm\n",
    "            with open(MODEL_PATH, 'wb') as file, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, unit_divisor=1024\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(block_size):\n",
    "                    file.write(data)\n",
    "                    pbar.update(len(data))\n",
    "\n",
    "            print(f\"Downloaded model to {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(\"Model file already exists. Skipping download.\")\n",
    "\n",
    "# Call the download_model function to download the model if necessary\n",
    "download_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"E:\\projects\\langchain-one\\models\\ggml-model-gpt4all-falcon-q4_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  E:\\\\projects\\\\langchain-one\\\\models\\\\ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=MODEL_PATH, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Boil water in a pot.\n",
      "2. Add loose leaf tea (e.g. green tea, jasmine tea) into the pot.\n",
      "3. Let it steep for 5-10 minutes.\n",
      "4. Strain the tea into another pot.\n",
      "5. Add milk and sweetener (e.g. sugar, honey) to taste.\n",
      "6. Heat the mixture on low heat until it's warm.\n",
      "7. Pour the tea into a cup and enjoy!"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\"How to make milk tea?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'text'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Day 1:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Red Fort\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Lotus Temple\n",
      "5. Visit Humayun's Tomb\n",
      "6. Visit Jama Masjid\n",
      "7. Visit Rajpath\n",
      "8. Visit Rashtrapati Bhavan\n",
      "9. Visit India Gate\n",
      "10. Visit Connaught Place\n",
      "11. Visit Laxmi Narayan Mandir\n",
      "12. Check out of hotel\n",
      "\n",
      "Day 2:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Lotus Temple\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Humayun's Tomb\n",
      "5. Visit Jama Masjid\n",
      "6. Visit Rajpath\n",
      "7. Visit Rashtrapati Bhavan\n",
      "8. Visit India Gate\n",
      "9. Visit Connaught Place\n",
      "10. Check out of hotel\n",
      "\n",
      "Day 3:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Lotus Temple\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Humayun's Tomb\n",
      "5. Visit Jama Masjid\n",
      "6. Visit Rajpath\n",
      "7. Visit Rashtrapati Bhavan\n",
      "8. Visit India Gate\n",
      "9. Check out of hotel\n",
      "\n",
      "Day 1:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Red Fort\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Lotus Temple\n",
      "5. Visit Humayun's Tomb\n",
      "6. Visit Jama Masjid\n",
      "7. Visit Rajpath\n",
      "8. Visit Rashtrapati Bhavan\n",
      "9. Visit India Gate\n",
      "10. Visit Connaught Place\n",
      "11. Visit Laxmi Narayan Mandir\n",
      "12. Check out of hotel\n",
      "\n",
      "Day 2:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Lotus Temple\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Humayun's Tomb\n",
      "5. Visit Jama Masjid\n",
      "6. Visit Rajpath\n",
      "7. Visit Rashtrapati Bhavan\n",
      "8. Visit India Gate\n",
      "9. Visit Connaught Place\n",
      "10. Check out of hotel\n",
      "\n",
      "Day 3:\n",
      "\n",
      "1. Check into hotel\n",
      "2. Visit Lotus Temple\n",
      "3. Visit Qutub Minar\n",
      "4. Visit Humayun's Tomb\n",
      "5. Visit Jama Masjid\n",
      "6. Visit Rajpath\n",
      "7. Visit Rashtrapati Bhavan\n",
      "8. Visit India Gate\n",
      "9. Check out of hotel\n"
     ]
    }
   ],
   "source": [
    "input = input(\"Ask me anything:\")\n",
    "output = llm_chain(str(input))\n",
    "answer = output['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked: What will be the iternary for New Delhi for 10 days?\n"
     ]
    }
   ],
   "source": [
    "print(f\"You asked: {output['question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
